{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Stock Screener Module](#toc1_)    \n",
    "  - [Prelimenary Imports and ENV variable definitions](#toc1_1_)    \n",
    "  - [Downloading/Scraping necessary information for the screening process](#toc1_2_)    \n",
    "    - [Obtaining the ETF holdings](#toc1_2_1_)    \n",
    "    - [Obtaining the failstodeliver information.](#toc1_2_2_)    \n",
    "    - [Obtaining 13f Forms](#toc1_2_3_)    \n",
    "  - [Creation of the main dataset](#toc1_3_)    \n",
    "    - [Collecting data from the submission table](#toc1_3_1_)    \n",
    "    - [Converting the Accession Numbers to Name of Issuers](#toc1_3_2_)    \n",
    "    - [Converting CUSIP to Ticker Symbols](#toc1_3_3_)    \n",
    "    - [Adding the remainder of the ticker symbols to the dataset](#toc1_3_4_)    \n",
    "  - [Optimizing the dataset and creating subset b](#toc1_4_)    \n",
    "  - [Creation of the subset c](#toc1_5_)    \n",
    "  - [Storing the ticker symbols into the cfs module](#toc1_6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Stock Screener Module](#toc0_)\n",
    "A software module that generates a subset (b) from a superset (a) of stock names based on a screening criteria. Additional sub-subsets (c), etc. of the subset are created by applying additional criterias (fundamental analysis, etc) to eventually generate one final subset (the target stock list).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Preliminary Imports and ENV variable definitions](#toc0_)\n",
    "Performing the necessary imports and constants.\n",
    "CIK_IDENTIFIER is a curated list of CIK ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import os\n",
    "import yfinance as yf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()\n",
    "FILE_PATH = r\"./dataset/\" \n",
    "\n",
    "\n",
    "CIK_IDENTIFIERS = [\n",
    "    '0001720792',\n",
    "    '0001099281',\n",
    "    '0001079114',\n",
    "    '0001112520',\n",
    "    '0001641864',\n",
    "    '0000846222',\n",
    "    '0001709323',\n",
    "    '0000732905',\n",
    "    '0000883965',\n",
    "    '0001067983',\n",
    "    '0001061768',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Downloading/Scraping necessary information for the screening process](#toc0_)\n",
    "In this section we download the needed information and store them into the /dataset folder for use by the ssm. First we define the current date in order to select the proper quarter and year for the ETFs and 13fs. Also we define the headers for the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "# Empty out directory\n",
    "files = [filename for filename in os.listdir(FILE_PATH) if not filename.startswith(\"README\")]\n",
    "for file in files:\n",
    "    os.remove(FILE_PATH+file)\n",
    "\n",
    "month = datetime.now().month\n",
    "quarter = 4 if int(month/4) == 0 else int(month/4)\n",
    "print(quarter)\n",
    "year = datetime.now().year\n",
    "\n",
    "headers = {\n",
    "    'Host': 'www.sec.gov', 'Connection': 'close',\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
    "}       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Obtaining the ETF holdings](#toc0_)\n",
    "In this section we obtain the ETF holdings from the specified urls, then write them into csv files located in /dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_high_div_etf = 'https://www.blackrock.com/us/individual/products/239563/ishares-high-dividend-etf/1464253357814.ajax?fileType=csv&fileName=HDV_holdings&dataType=fund'\n",
    "url_core_div_etf = 'https://www.ishares.com/us/products/291387/fund/1467271812596.ajax?fileType=csv&fileName=DIVB_holdings&dataType=fund'\n",
    "\n",
    "r = requests.get(url_high_div_etf, allow_redirects=True)\n",
    "with open(FILE_PATH + 'HDV_holdings.csv', 'wb+') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "r = requests.get(url_core_div_etf, allow_redirects=True)\n",
    "with open(FILE_PATH + 'DIVB_holdings.csv', 'wb+') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "with open(FILE_PATH + 'HDV_holdings.csv', 'r', encoding='utf-8-sig') as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "with open(FILE_PATH + 'HDV_holdings.csv', 'w', encoding='utf-8-sig') as fp:\n",
    "    for i, line in enumerate(lines):\n",
    "        if i<9: continue\n",
    "        fp.write(line)\n",
    "\n",
    "with open(FILE_PATH + 'DIVB_holdings.csv', 'r', encoding='utf-8-sig') as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "with open(FILE_PATH + 'DIVB_holdings.csv', 'w', encoding='utf-8-sig') as fp:\n",
    "    for i, line in enumerate(lines):\n",
    "        if i<9: continue\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Obtaining the failstodeliver information.](#toc0_)\n",
    "Next we get the fails to deliver data. We do this in order to get the ticker symbol from the CUSIP which is extracted later from the 13fs. The urls for the different files are the same and only vary in month and year. For example if today was January 1st 2022 then the url would be:\n",
    "`https://www.sec.gov/files/data/fails-deliver-data/cnsfails202201a.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmonth = month\n",
    "fyear = year\n",
    "for i in range(3):\n",
    "    fmonth = int(fmonth)-1\n",
    "    fyear = fyear if fmonth != 0 else fyear-1\n",
    "    if fmonth == 0:\n",
    "        str_month = '12'\n",
    "        fmonth = 12\n",
    "    elif fmonth < 10:\n",
    "        str_month = '0' + str(fmonth)\n",
    "    else:\n",
    "        str_month = str(fmonth)\n",
    "\n",
    "    print(f\"month {str_month} year {fyear}\")\n",
    "    fails_deliver_url = f'https://www.sec.gov/files/data/fails-deliver-data/cnsfails{fyear}{str_month}a.zip'\n",
    "    r = requests.get(fails_deliver_url, headers=headers, allow_redirects=True)\n",
    "    z = zipfile.ZipFile(BytesIO(r.content))\n",
    "    z.extract(f'cnsfails{fyear}{str_month}a', FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Obtaining 13f Forms](#toc0_)\n",
    "Finally we get the 13f forms, we only get the last 3 most recent quarters.\n",
    "\n",
    "Similar to the fails to deliver data the urls follow a similar format, they take on the form of: \n",
    "\n",
    "https://www.sec.gov/files/structureddata/data/form-13f-data-sets/{year}q{quarter}_form13f.zip\n",
    "\n",
    "So we need to first determine the year and convert the month to the current quarter.\n",
    "\n",
    "Then we get the 3 most recent 13f data sets then finally we need to also extract the 2 files desired from the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if quarter == 4: # q4 comes out in the new year so most recent data will be in last year for the 13f data.\n",
    "    year -= 1\n",
    "\n",
    "files = 3 # Last {files} quarters of 13f data\n",
    "i = 0 # iterator not pythonic but might be mutated\n",
    "while i < files:\n",
    "    \n",
    "    if i > 0:\n",
    "        quarter -= 1\n",
    "        if quarter <= 0:\n",
    "            quarter = 4\n",
    "            year -= 1\n",
    "            \n",
    "    url = f'https://www.sec.gov/files/structureddata/data/form-13f-data-sets/{year}q{quarter}_form13f.zip'\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, allow_redirects=True)\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        zipinfos = z.infolist()\n",
    "        for zipinfo in zipinfos:\n",
    "            if \"INFOTABLE\" in zipinfo.filename:\n",
    "                zipinfo.filename = f'INFOTABLE_{year}_q{quarter}.tsv'\n",
    "                z.extract(zipinfo, FILE_PATH)\n",
    "            elif \"SUBMISSION\" in zipinfo.filename:\n",
    "                zipinfo.filename = f'SUBMISSION_{year}_q{quarter}.tsv'\n",
    "                z.extract(zipinfo, FILE_PATH)\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"13f of specific quarter not present\") \n",
    "        files += 1 # Get next most recent\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Creation of the main dataset](#toc0_)\n",
    "We will now compile all the data we just obtained from the 13f and holdings into one major set of data. To do this we must first gather the data from the 13f and convert them into ticker symbols.\n",
    "\n",
    "### <a id='toc1_3_1_'></a>[Collecting data from the submission table](#toc0_)\n",
    "From the SUBMISSION table we fetch a list of ACCESSION_NUMBER(s) using the CIK identifiers defined at the start of the program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_submissions = []\n",
    "\n",
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if filename.startswith(\"SUBMISSION\")]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file, 'r', encoding='utf-8') as q:\n",
    "        for submission in csv.DictReader(q, delimiter=\"\\t\"):\n",
    "            if submission[\"CIK\"] in CIK_IDENTIFIERS:\n",
    "                picked_submissions.append(submission[\"ACCESSION_NUMBER\"])\n",
    "\n",
    "pprint(len(picked_submissions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Converting the Accession Numbers to Name of Issuers](#toc0_)\n",
    "From the INFOTABLE fetch a list of NAMEOFISSUER(s) using the ACCESSION_NUMBER(s) created in the previous cell. We then will use CUSIP(s) to map between brokers since it is unique where names differ slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_of_issuers = set()\n",
    "\n",
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if filename.startswith(\"INFOTABLE\")]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file, 'r', encoding='utf-8') as q:\n",
    "        for entry in csv.DictReader(q, delimiter=\"\\t\"):\n",
    "            if entry[\"ACCESSION_NUMBER\"] in picked_submissions:\n",
    "                names_of_issuers.add(entry[\"CUSIP\"].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_3_'></a>[Converting CUSIP to Ticker Symbols](#toc0_)\n",
    "\n",
    "Now we need to convert the CUSIP to tickers, we will do this using the cnsfails to deliver files to fetch info about a holding by it's CUSIP ID. \n",
    "\n",
    "In this step we lose a bit of the data in this step as we fail to be able to convert all tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = set()\n",
    "\n",
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if filename.startswith(\"cnsfail\")]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file,'r') as f:\n",
    "        for entry in csv.DictReader(f, delimiter=\"|\"):\n",
    "            if entry['CUSIP'] in names_of_issuers: \n",
    "                tickers.add(entry['SYMBOL'])\n",
    "                names_of_issuers.remove(entry['CUSIP'])\n",
    "    \n",
    "pprint(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_4_'></a>[Adding the remainder of the ticker symbols to the dataset](#toc0_)\n",
    "Adding the rest of the ticker symbols to the set from the other datasets. To do this we simple append all the ticker symbols into the list.\n",
    "\n",
    "However before anything delete the first 10 rows of the csv files {DIVB_holdings, HDV_holdings} as it messed up the parsing for DictReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if \"holdings\" in filename]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file,'r', encoding='utf-8-sig') as f:\n",
    "        for entry in csv.DictReader(f, delimiter=\",\"):\n",
    "            entry.keys()\n",
    "            tickers.add(entry[\"Ticker\"])\n",
    "        \n",
    "pprint(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Optimizing the dataset and creating subset b](#toc0_)\n",
    "At this point set A is complete and we can create subsets of it for the next few steps but first we will parse it a bit to speed up the process a bit. So we simply remove all tickers that do not offer dividends, we do this by simply checking if dividend rate is defined. We pull the data by scraping with yfinance.\n",
    "\n",
    "We will call this subset of a, subset b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import HTTPError\n",
    "\n",
    "arr_A = list(tickers)\n",
    "\n",
    "ticker_objs = list(yf.Tickers(arr_A).tickers.values())\n",
    "arr_B = []\n",
    "for ticker in ticker_objs:\n",
    "    try:\n",
    "        if 'dividendRate' in ticker.info.keys():\n",
    "            arr_B.append(ticker.info[\"symbol\"])\n",
    "    except HTTPError:\n",
    "        print(f\"Ticker not found, removed from subset.\")\n",
    "        continue\n",
    "\n",
    "pprint(arr_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Creation of the subset c](#toc0_)\n",
    "\n",
    "From subset (b) remove all names that have a high business risk, a debt to equity ratio greater than 1.5, sub-subset (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_objs = list(yf.Tickers(arr_B).tickers.values())\n",
    "\n",
    "\n",
    "arr_C = []\n",
    "\n",
    "for ticker in ticker_objs:\n",
    "    try:\n",
    "        balance_sheet = list(ticker.balancesheet.to_dict().values())[0] # get most recent data\n",
    "        liabilities = balance_sheet['Total Liabilities Net Minority Interest']\n",
    "        assets = balance_sheet['Total Assets']\n",
    "        debtToEquity = abs( liabilities / (assets - liabilities) )\n",
    "    except ZeroDivisionError:\n",
    "        print(ticker.info[\"symbol\"]) # if this is close to 0 then equity to debt ratio is near inf \n",
    "        continue                     # So we skip it.\n",
    "    except KeyError:\n",
    "        print('Missing Balance Sheet Info')\n",
    "        print(ticker.info[\"symbol\"])\n",
    "    if debtToEquity <= 1.5:\n",
    "        arr_C.append(ticker.info[\"symbol\"])\n",
    "\n",
    "print(len(arr_C))\n",
    "pprint(arr_C)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Storing the ticker symbols into the cfs module](#toc0_)\n",
    "Store subset into a file for the cfs module to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../cfs_module/subset_c.txt', 'w') as f:\n",
    "    f.write('\\n'.join(arr_C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
