{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prelimenary Imports and ENV variable definitions\n",
    "import csv\n",
    "import os\n",
    "import yfinance as yf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()\n",
    "FILE_PATH = r\"./dataset/\" \n",
    "\n",
    "\n",
    "CIK_IDENTIFIERS = [\n",
    "    '0001720792',\n",
    "    '0001099281',\n",
    "    '0001079114',\n",
    "    '0001112520',\n",
    "    '0001641864',\n",
    "    '0000846222',\n",
    "    '0001709323',\n",
    "    '0000732905',\n",
    "    '0000883965',\n",
    "    '0001067983',\n",
    "    '0001061768',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading neccessary files, a lot of py magic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m z\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnsfails\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfyear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, FILE_PATH)\n\u001b[0;32m     62\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(fails_deliver_url_b, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 63\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m z\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnsfails\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfyear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, FILE_PATH)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quarter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m: \u001b[38;5;66;03m# q4 comes out in the new year so most recent data will be in last year for the 13f data.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\zipfile.py:1269\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1269\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\zipfile.py:1336\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "# Empty out directory\n",
    "files = [filename for filename in os.listdir(FILE_PATH) if not filename.startswith(\"README\")]\n",
    "for file in files:\n",
    "    os.remove(FILE_PATH+file)\n",
    "\n",
    "month = datetime.now().month\n",
    "quarter = 4 if int(month/4) == 0 else int(month/4)\n",
    "print(quarter)\n",
    "year = datetime.now().year\n",
    "\n",
    "fmonth = month-1\n",
    "fyear = year if fmonth != 0 else year-1\n",
    "if fmonth == 0:\n",
    "    fmonth = '12'\n",
    "elif fmonth < 10:\n",
    "    fmonth = '0' + str(fmonth)\n",
    "\n",
    "\n",
    "\n",
    "fails_deliver_url = f'https://www.sec.gov/files/data/fails-deliver-data/cnsfails{fyear}{fmonth}a.zip'\n",
    "url_high_div_etf = 'https://www.blackrock.com/us/individual/products/239563/ishares-high-dividend-etf/1464253357814.ajax?fileType=csv&fileName=HDV_holdings&dataType=fund'\n",
    "url_core_div_etf = 'https://www.ishares.com/us/products/291387/fund/1467271812596.ajax?fileType=csv&fileName=DIVB_holdings&dataType=fund'\n",
    "headers = {\n",
    "    'Host': 'www.sec.gov', 'Connection': 'close',\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
    "}\n",
    "\n",
    "r = requests.get(url_high_div_etf, allow_redirects=True)\n",
    "with open(FILE_PATH + 'HDV_holdings.csv', 'wb+') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "r = requests.get(url_core_div_etf, allow_redirects=True)\n",
    "with open(FILE_PATH + 'DIVB_holdings.csv', 'wb+') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "with open(FILE_PATH + 'HDV_holdings.csv', 'r', encoding='utf-8-sig') as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "with open(FILE_PATH + 'HDV_holdings.csv', 'w', encoding='utf-8-sig') as fp:\n",
    "    for i, line in enumerate(lines):\n",
    "        if i<9: continue\n",
    "        fp.write(line)\n",
    "\n",
    "with open(FILE_PATH + 'DIVB_holdings.csv', 'r', encoding='utf-8-sig') as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "with open(FILE_PATH + 'DIVB_holdings.csv', 'w', encoding='utf-8-sig') as fp:\n",
    "    for i, line in enumerate(lines):\n",
    "        if i<9: continue\n",
    "        fp.write(line)\n",
    "\n",
    "r = requests.get(fails_deliver_url, headers=headers, allow_redirects=True)\n",
    "z = zipfile.ZipFile(BytesIO(r.content))\n",
    "z.extract(f'cnsfails{fyear}{fmonth}a', FILE_PATH)\n",
    "\n",
    "\n",
    "if quarter == 4: # q4 comes out in the new year so most recent data will be in last year for the 13f data.\n",
    "    year -= 1\n",
    "\n",
    "files = 3 # Last {files} quarters of 13f data\n",
    "i = 0 # iterator not pythonic but might be mutated\n",
    "while i < files:\n",
    "    \n",
    "    if i > 0:\n",
    "        quarter -= 1\n",
    "        if quarter <= 0:\n",
    "            quarter = 4\n",
    "            year -= 1\n",
    "            \n",
    "    url = f'https://www.sec.gov/files/structureddata/data/form-13f-data-sets/{year}q{quarter}_form13f.zip'\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, allow_redirects=True)\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        zipinfos = z.infolist()\n",
    "        for zipinfo in zipinfos:\n",
    "            if \"INFOTABLE\" in zipinfo.filename:\n",
    "                zipinfo.filename = f'INFOTABLE_{year}_q{quarter}.tsv'\n",
    "                z.extract(zipinfo, FILE_PATH)\n",
    "            elif \"SUBMISSION\" in zipinfo.filename:\n",
    "                zipinfo.filename = f'SUBMISSION_{year}_q{quarter}.tsv'\n",
    "                z.extract(zipinfo, FILE_PATH)\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"13f of specific quarter not present\") \n",
    "        files += 1 # Get next most recent\n",
    "    i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the SUBMISSION table fetch a list of ACCESSION_NUMBER(s) using the CIK identifiers in table A-1 (Appendix).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_submissions = []\n",
    "\n",
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if filename.startswith(\"SUBMISSION\")]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file, 'r', encoding='utf-8') as q:\n",
    "        for submission in csv.DictReader(q, delimiter=\"\\t\"):\n",
    "            if submission[\"CIK\"] in CIK_IDENTIFIERS:\n",
    "                picked_submissions.append(submission[\"ACCESSION_NUMBER\"])\n",
    "\n",
    "pprint(len(picked_submissions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the INFOTABLE fetch a list of NAMEOFISSUER(s) using the ACCESSION_NUMBER(s) created in (b). Use CUSIP(s) to map between brokers since it is unique where names differ slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_of_issuers = set()\n",
    "\n",
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if filename.startswith(\"INFOTABLE\")]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file, 'r', encoding='utf-8') as q:\n",
    "        for entry in csv.DictReader(q, delimiter=\"\\t\"):\n",
    "            if entry[\"ACCESSION_NUMBER\"] in picked_submissions:\n",
    "                names_of_issuers.add(entry[\"CUSIP\"].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the CUSIP to tickers, we will do this using the polygon API to fetch info about a holding by it's CUSIP ID. \n",
    "\n",
    "Simply download the last 2 most recent file from https://www.sec.gov/data/foiadocsfailsdatahtm and store in dataset folder.\n",
    "\n",
    "In this step we lose about 12% of the dataset... Unsure if there is a better way to resolve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = set()\n",
    "\n",
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if filename.startswith(\"cnsfail\")]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file,'r') as f:\n",
    "        for entry in csv.DictReader(f, delimiter=\"|\"):\n",
    "            if entry['CUSIP'] in names_of_issuers: \n",
    "                tickers.add(entry['SYMBOL'])\n",
    "                names_of_issuers.remove(entry['CUSIP'])\n",
    "    \n",
    "pprint(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the rest of the ticker symbols to the set from the other datasets.\n",
    "\n",
    "Before anything delete the first 10 rows of the csv files {DIVB_holdings, HDV_holdings} as it messed up the parsing for DictReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixed = [filename for filename in os.listdir(FILE_PATH) if \"holdings\" in filename]\n",
    "print(prefixed)\n",
    "\n",
    "for file in prefixed:\n",
    "    with open(FILE_PATH + file,'r', encoding='utf-8-sig') as f:\n",
    "        for entry in csv.DictReader(f, delimiter=\",\"):\n",
    "            entry.keys()\n",
    "            tickers.add(entry[\"Ticker\"])\n",
    "        \n",
    "pprint(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From set A, remove all tickers that do not offer dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import HTTPError\n",
    "\n",
    "\n",
    "arr_A = list(tickers)\n",
    "\n",
    "ticker_objs = list(yf.Tickers(arr_A).tickers.values())\n",
    "arr_B = []\n",
    "for ticker in ticker_objs:\n",
    "    try:\n",
    "        if 'dividendRate' in ticker.info.keys():\n",
    "            arr_B.append(ticker.info[\"symbol\"])\n",
    "    except HTTPError:\n",
    "        print(f\"Ticker not found, removed from subset.\")\n",
    "        continue\n",
    "\n",
    "pprint(arr_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From subset (b) remove all names that have a high business risk, a debt to equity ratio greater than 1.5, sub-subset (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_objs = list(yf.Tickers(arr_B).tickers.values())\n",
    "\n",
    "\n",
    "arr_C = []\n",
    "\n",
    "for ticker in ticker_objs:\n",
    "    try:\n",
    "        balance_sheet = list(ticker.balancesheet.to_dict().values())[0] # get most recent data\n",
    "        liabilities = balance_sheet['Total Liabilities Net Minority Interest']\n",
    "        assets = balance_sheet['Total Assets']\n",
    "        debtToEquity = abs( liabilities / (assets - liabilities) )\n",
    "    except ZeroDivisionError:\n",
    "        print(ticker.info[\"symbol\"]) # if this is close to 0 then equity to debt ratio is near inf \n",
    "        continue                     # So we skip it.\n",
    "    except KeyError:\n",
    "        print('Missing Balance Sheet Info')\n",
    "        print(ticker.info[\"symbol\"])\n",
    "    if debtToEquity <= 1.5:\n",
    "        arr_C.append(ticker.info[\"symbol\"])\n",
    "\n",
    "print(len(arr_C))\n",
    "pprint(arr_C)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store subset into a file for the cfs module to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../cfs_module/subset_c.txt', 'w') as f:\n",
    "    f.write('\\n'.join(arr_C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
